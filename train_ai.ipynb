{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_multiplier = 10   # increase the size of the subset by taking random composition of CCD\n",
    "epoch = 60                # number of epochs for each AI\n",
    "n = 25                    # number of loops for the genertic algorithm\n",
    "K = 3                     # number of folds (= number of independant AI per generation)\n",
    "\n",
    "train_on = \"triplets\"     # \"triplets\" or \"blocks\"\n",
    "function = \"tan\"          # \"tan\" or \"square\"\n",
    "train_with = \"parameters\" # \"parameters\" or \"points\" TODO\n",
    "\n",
    "train_prop = 0.7          # proportion of the dataset used for training\n",
    "\n",
    "weights = [7.686424453317564,2.8124992994763316,0.448360122048755,4.727911837705288] # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys, copy\n",
    "import data_io\n",
    "import utils.archive as archive\n",
    "import numpy             as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas            as pd\n",
    "import tensorflow        as tf\n",
    "from   IPython.display   import display\n",
    "from   tensorflow        import keras\n",
    "from   classes.block     import Block\n",
    "from   classes.triplet   import Triplet\n",
    "from   classes.shot      import Shot\n",
    "from   classes.ccd       import CCD\n",
    "from   classes.rate      import Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(x_train,y_train):\n",
    "    # mse = tf.keras.losses.MeanSquaredError()\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input((len(x_train[0]),), name=\"InputLayer\"))\n",
    "    model.add(keras.layers.Dense(64, activation='relu', name='Dense_n1'))\n",
    "    model.add(keras.layers.Dense(64, activation='relu', name='Dense_n2'))\n",
    "    model.add(keras.layers.Dense(64, activation='relu', name='Dense_n3'))\n",
    "    model.add(keras.layers.Dense(len(y_train), name='Output'))\n",
    "    model.compile(optimizer = 'adam', loss = 'mse', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of efficiency functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tan\n",
    "def ft(m,a,b,c,d):\n",
    "    return a/4 * (1-np.tanh((m-b)/c)) * (1-np.tanh((m-b)/d))\n",
    "\n",
    "# square\n",
    "def fs(m,a,b,c,d):\n",
    "    return (a-b*(m-21)**2) / (1+np.exp((m-c)/d))\n",
    "\n",
    "# magnitude range\n",
    "m = np.linspace(21,25.5,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data and creating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 2013AE is already loaded.\n",
      "Block 2013AO is already loaded.\n",
      "Block 2013BL is already loaded.\n",
      "Block 2014BH is already loaded.\n",
      "Block 2015AM is already loaded.\n",
      "Block 2015AP is already loaded.\n",
      "Block 2015BC is already loaded.\n",
      "Block 2015BD is already loaded.\n",
      "Block 2015BS is already loaded.\n",
      "Block 2015BT is already loaded.\n",
      "1610 vectors containing 1728 inputs and 4 outputs\n"
     ]
    }
   ],
   "source": [
    "# Loading data\n",
    "data_io.loadAll() # comment if already loaded to spare time\n",
    "\n",
    "# Formating data in data set usable by the AI\n",
    "data, outputs = data_io.get_ai_ready(items = Triplet.all, func=function,subsets_per_block=subset_multiplier)\n",
    "\n",
    "print(len(data), \"vectors containing\", len(data[0])-outputs, \"inputs and\", outputs, \"outputs\")\n",
    "\n",
    "# Normalization\n",
    "mean = data[:,:-outputs].mean()\n",
    "std  = data[:,:-outputs].std()\n",
    "data[:,:-outputs] = (data[:,:-outputs] - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting a random test item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test item: E-2-1\n"
     ]
    }
   ],
   "source": [
    "if train_on == \"blocks\":\n",
    "    test_item = iter(Block.all.values())[np.random.randint(0,len(Block.all)-1)]\n",
    "if train_on == \"triplets\":\n",
    "    tripletList = []\n",
    "    for rate in Rate.all:\n",
    "        if type(rate.parent) == Triplet and rate.parent.id not in tripletList:\n",
    "            tripletList.append(rate.parent.id)\n",
    "    test_item = Triplet.all[tripletList[np.random.randint(0,len(tripletList)-1)]]\n",
    "\n",
    "print(\"Test item:\",test_item.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on test item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(test_item, model):\n",
    "    if test_item.to_ai_ready(func=\"tan\") is not None:\n",
    "        new_data,outputs = test_item.to_ai_ready(func=\"tan\")\n",
    "    else:\n",
    "        new_data,outputs = test_item.to_ai_ready(func=\"square\")\n",
    "\n",
    "    new_x = new_data[:-outputs]\n",
    "    new_y = new_data[-outputs:]\n",
    "    new_x = (new_x - mean) / std\n",
    "\n",
    "    new_x=np.array(new_x).reshape(1,len(new_x))\n",
    "\n",
    "    predictions = model.predict(new_x)\n",
    "\n",
    "    print(f\"Prediction : {predictions[0]}\")\n",
    "\n",
    "    # Plotting the result\n",
    "\n",
    "    plt.subplot(int(np.ceil(np.sqrt(n))),int(np.ceil(np.sqrt(n))),i+1)\n",
    "    plt.plot(m, ft(m,*predictions[0]),                     label=\"Machine Learning\")\n",
    "    plt.plot(m, fs(m,new_y[0],new_y[1],new_y[2],new_y[3]), label=\"Excpected\")\n",
    "    plt.grid()\n",
    "\n",
    "    if i == 0:                    plt.title(f\"Predition for {train_on} {test_item.id}\")\n",
    "    if i+1>n-np.ceil(np.sqrt(n)): plt.xlabel(\"Magnitude\")\n",
    "    if i%np.ceil(np.sqrt(n))==0:  plt.ylabel(\"Efficiency\")\n",
    "    if i==0:                      plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cenerating folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds(K, data, train_prop, outputs):\n",
    "\n",
    "    # Creating folds\n",
    "    folds = []\n",
    "    for j in range(K):\n",
    "        folds.append(data[j::K])\n",
    "\n",
    "    # Splitting data for training and test...\n",
    "    Xt_list = []; Yt_list = []; Xv_list = []; Yv_list = []\n",
    "    for j, fold in enumerate(folds):\n",
    "        train_sets = int(len(fold)*train_prop)\n",
    "        index = np.zeros(len(fold),dtype=bool)\n",
    "        index[:train_sets] = True\n",
    "        np.random.shuffle(index)\n",
    "\n",
    "        data_train = fold[index]\n",
    "        data_test  = fold[~index]\n",
    "\n",
    "        Xt_list.append(data_train[:,:-outputs])\n",
    "        Yt_list.append(data_train[:,-outputs:])\n",
    "        Xv_list.append(data_test [:,:-outputs])\n",
    "        Yv_list.append(data_test [:,-outputs:])\n",
    "\n",
    "    # Composing training data using fold != j\n",
    "    for j in range(K):\n",
    "        Xt_list[j] = np.concatenate(np.array(Xt_list)[np.arange(len(Xt_list))!=j])\n",
    "        Yt_list[j] = np.concatenate(np.array(Yt_list)[np.arange(len(Yt_list))!=j])\n",
    "\n",
    "    return np.array(Xt_list), np.array(Yt_list), np.array(Xv_list), np.array(Yv_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(Xt,Yt,Xv,Yv,epoch):\n",
    "    h = models[j].fit(Xt, Yt, epochs = epoch, verbose = 0)        #, validation_data = (x_test[j], y_test[j])\n",
    "    s = models[j].evaluate(Xv, Yv, verbose=0)\n",
    "    return h,s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keeping only the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model(generation, models, scores, lastScore):\n",
    "    minScore = lastScore\n",
    "    output.write(f\"{generation+1}, \")\n",
    "    if minScore is None: minScore = scores[0][0]\n",
    "    if model is None: model = models[0]\n",
    "    average = 0\n",
    "    for j,s in enumerate(scores):\n",
    "        output.write(f\"{s[0]}, \") \n",
    "        average += s[0]/K\n",
    "        if s[0] < minScore:\n",
    "            minScore = s[0]\n",
    "            model = models[j]\n",
    "    output.write(f\"{average}, {minScore}\\n\")\n",
    "    return model, minScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training neural nework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Generation 1/25\n",
      "Shape of Xt: (750, 1728)th fold 0 as test...\n",
      "Shape of Yt: (750, 4)\n",
      "Shape of Xv: (162, 1728)\n",
      "Shape of Yv: (162, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vince\\AppData\\Local\\Temp\\ipykernel_26336\\3176997729.py:26: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  Xt_list[j] = np.concatenate(np.array(Xt_list)[np.arange(len(Xt_list))!=j])\n",
      "C:\\Users\\vince\\AppData\\Local\\Temp\\ipykernel_26336\\3176997729.py:27: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  Yt_list[j] = np.concatenate(np.array(Yt_list)[np.arange(len(Yt_list))!=j])\n",
      "C:\\Users\\vince\\AppData\\Local\\Temp\\ipykernel_26336\\3176997729.py:29: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(Xt_list), np.array(Yt_list), np.array(Xv_list), np.array(Yv_list)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\vince\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\vince\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\vince\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\vince\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py\", line 890, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\vince\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py\", line 948, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\vince\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\vince\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\losses.py\", line 139, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\vince\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\losses.py\", line 243, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\vince\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\losses.py\", line 1327, in mean_squared_error\n        return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n\n    ValueError: Dimensions must be equal, but are 750 and 4 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](sequential_1/Output/BiasAdd, IteratorGetNext:1)' with input shapes: [?,750], [?,4].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mf:\\Vincent\\Dev\\Compuphys\\M1-TNO_Detection_Efficiency\\train_ai.ipynb Cell 22'\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Vincent/Dev/Compuphys/M1-TNO_Detection_Efficiency/train_ai.ipynb#ch0000011?line=40'>41</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mShape of Xv:\u001b[39m\u001b[39m\"\u001b[39m,Xv\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Vincent/Dev/Compuphys/M1-TNO_Detection_Efficiency/train_ai.ipynb#ch0000011?line=41'>42</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mShape of Yv:\u001b[39m\u001b[39m\"\u001b[39m,Yv\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/Vincent/Dev/Compuphys/M1-TNO_Detection_Efficiency/train_ai.ipynb#ch0000011?line=43'>44</a>\u001b[0m res, score \u001b[39m=\u001b[39m train(Xt, Yt, Xv, Yv, epoch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Vincent/Dev/Compuphys/M1-TNO_Detection_Efficiency/train_ai.ipynb#ch0000011?line=44'>45</a>\u001b[0m history\u001b[39m.\u001b[39mappend(res)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Vincent/Dev/Compuphys/M1-TNO_Detection_Efficiency/train_ai.ipynb#ch0000011?line=45'>46</a>\u001b[0m scores\u001b[39m.\u001b[39mappend(score)\n",
      "\u001b[1;32mf:\\Vincent\\Dev\\Compuphys\\M1-TNO_Detection_Efficiency\\train_ai.ipynb Cell 18'\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(Xt, Yt, Xv, Yv, epoch)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Vincent/Dev/Compuphys/M1-TNO_Detection_Efficiency/train_ai.ipynb#ch0000020?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(Xt,Yt,Xv,Yv,epoch):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/Vincent/Dev/Compuphys/M1-TNO_Detection_Efficiency/train_ai.ipynb#ch0000020?line=1'>2</a>\u001b[0m     h \u001b[39m=\u001b[39m models[j]\u001b[39m.\u001b[39;49mfit(Xt, Yt, epochs \u001b[39m=\u001b[39;49m epoch, verbose \u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m)        \u001b[39m#, validation_data = (x_test[j], y_test[j])\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Vincent/Dev/Compuphys/M1-TNO_Detection_Efficiency/train_ai.ipynb#ch0000020?line=2'>3</a>\u001b[0m     s \u001b[39m=\u001b[39m models[j]\u001b[39m.\u001b[39mevaluate(Xv, Yv, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Vincent/Dev/Compuphys/M1-TNO_Detection_Efficiency/train_ai.ipynb#ch0000020?line=3'>4</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m h,s\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file3qs_d_t4.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\vince\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\vince\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\vince\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\vince\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py\", line 890, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\vince\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py\", line 948, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\vince\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\vince\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\losses.py\", line 139, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\vince\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\losses.py\", line 243, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\vince\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\losses.py\", line 1327, in mean_squared_error\n        return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n\n    ValueError: Dimensions must be equal, but are 750 and 4 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](sequential_1/Output/BiasAdd, IteratorGetNext:1)' with input shapes: [?,750], [?,4].\n"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "minScore = None\n",
    "path = archive.new(name = archive.description(M = subset_multiplier, N = n, K = K, E = epoch))\n",
    "output = open(f\"{path}/ouput.csv\",\"w\")\n",
    "\n",
    "output.write(\"Generation, \")\n",
    "for j in np.arange(K): output.write(f\"Score of model {j}, \")\n",
    "output.write(\"Average, Score retained\\n\")\n",
    "\n",
    "# Loop over generations (genetic algorithm)\n",
    "for i in range(n):\n",
    "  \n",
    "  print(f\"üîÅ Generation {i+1}/{n}\")\n",
    "\n",
    "  np.random.shuffle(data)\n",
    "\n",
    "  Xt_list, Yt_list, Xv_list, Yv_list = create_folds(K, data, train_prop, outputs)\n",
    "  \n",
    "\n",
    "  ################################################################################\n",
    "  # Training K models independently\n",
    "\n",
    "  models = []; history = []; scores = []\n",
    "  for j in range(K):\n",
    "\n",
    "    print(f\"üèÉ‚Äç‚ôÄÔ∏è Training model  with fold {j} as test...\", end=\"\\r\")\n",
    "\n",
    "    # Sub sets for this fold\n",
    "\n",
    "    Xt, Yt, Xv, Yv = Xt_list[j], Yt_list[j], Xv_list[j], Yv_list[j]\n",
    "\n",
    "    # Getting new model if it's the first generation, and the old one if not\n",
    "\n",
    "    if len(models) < j+1: models.append(get_model(Xt,Yt))\n",
    "    else: models.append(model)\n",
    "\n",
    "    # Training models\n",
    "\n",
    "    print(\"\\nShape of Xt:\",Xt.shape)\n",
    "    print(\"Shape of Yt:\",Yt.shape)\n",
    "    print(\"Shape of Xv:\",Xv.shape)\n",
    "    print(\"Shape of Yv:\",Yv.shape)\n",
    "\n",
    "    res, score = train(Xt, Yt, Xv, Yv, epoch)\n",
    "    history.append(res)\n",
    "    scores.append(score)\n",
    "\n",
    "  # Keeping the best one\n",
    "\n",
    "  model, lastScore = get_best_model(i,models, scores, lastScore)\n",
    "\n",
    "  # Making new prediction\n",
    "\n",
    "  print(\"üîÆ Prediction...\")\n",
    "  prediction()\n",
    "\n",
    "################################################################################\n",
    "# Saving results\n",
    "\n",
    "output.close()\n",
    "plt.savefig(f\"{path}/tno_efficiency_rate.png\")\n",
    "model.save(f\"{path}/model.ckpt\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e0d398e7c3cc3d2b8386dfea47f5eae3378d5d39db7e2c8ef87e93246db0bfd2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
